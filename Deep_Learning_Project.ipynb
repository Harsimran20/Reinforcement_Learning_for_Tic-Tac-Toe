{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761941b1-2994-47bc-b398-1260baab61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0901c32-dd36-45aa-bc66-b2db19f2bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9, dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.board[action] != 0 or self.done:\n",
    "            return self.board.copy(), -10, True  # Invalid move\n",
    "        self.board[action] = self.current_player\n",
    "        self.check_winner()\n",
    "        reward = 1 if self.winner == self.current_player else 0\n",
    "        self.current_player = 3 - self.current_player\n",
    "        return self.board.copy(), reward, self.done\n",
    "\n",
    "    def check_winner(self):\n",
    "        combos = [(0,1,2),(3,4,5),(6,7,8),\n",
    "                  (0,3,6),(1,4,7),(2,5,8),\n",
    "                  (0,4,8),(2,4,6)]\n",
    "        for (a,b,c) in combos:\n",
    "            if self.board[a] == self.board[b] == self.board[c] != 0:\n",
    "                self.done = True\n",
    "                self.winner = self.board[a]\n",
    "                return\n",
    "        if np.all(self.board != 0):\n",
    "            self.done = True\n",
    "\n",
    "    def render(self):\n",
    "        symbols = [' ', 'X', 'O']\n",
    "        print(\"\\nBoard:\")\n",
    "        for i in range(0, 9, 3):\n",
    "            print(' | '.join(symbols[self.board[i + j]] for j in range(3)))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390e5805-f727-4b2f-a177-b9d49132b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(board):\n",
    "    return ''.join(map(str, board))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa659f35-ca1f-4ead-b037-1159ac2ec10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return self.q_table.setdefault(state, np.zeros(9))\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        qs = self.get_qs(state)\n",
    "        qs = np.array([qs[a] if a in available_actions else -np.inf for a in range(9)])\n",
    "        return int(np.argmax(qs))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done, available_actions):\n",
    "        current_q = self.get_qs(state)[action]\n",
    "        max_future_q = 0 if done else max([self.get_qs(next_state)[a] for a in available_actions], default=0)\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_future_q - current_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f19677a-619f-43d3-9e65-282d87bf9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes=50000):\n",
    "    env = TicTacToe()\n",
    "    agent = QLearningAgent()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = encode_state(env.reset())\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            available = env.available_actions()\n",
    "            action = agent.choose_action(state, available)\n",
    "            next_state_arr, reward, done = env.step(action)\n",
    "            next_state = encode_state(next_state_arr)\n",
    "            agent.update(state, action, reward, next_state, done, env.available_actions())\n",
    "            state = next_state\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        if (episode + 1) % 5000 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d54a0e34-d60a-45a2-a9ab-4aae2d54e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, games=10):\n",
    "    env = TicTacToe()\n",
    "    win, lose, draw = 0, 0, 0\n",
    "\n",
    "    for _ in range(games):\n",
    "        state = encode_state(env.reset())\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            available = env.available_actions()\n",
    "            action = agent.choose_action(state, available)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = encode_state(next_state)\n",
    "\n",
    "            if not done:\n",
    "                opp_action = random.choice(env.available_actions())\n",
    "                _, _, done = env.step(opp_action)\n",
    "                state = encode_state(env.board)\n",
    "\n",
    "        if env.winner == 1:\n",
    "            win += 1\n",
    "        elif env.winner == 2:\n",
    "            lose += 1\n",
    "        else:\n",
    "            draw += 1\n",
    "\n",
    "    print(f\"Test results: {win} Wins, {lose} Losses, {draw} Draws\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6b3b56-7251-475c-91e9-c2e56e3a31b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000/50000 | Epsilon: 0.010\n",
      "Episode 10000/50000 | Epsilon: 0.010\n",
      "Episode 15000/50000 | Epsilon: 0.010\n",
      "Episode 20000/50000 | Epsilon: 0.010\n",
      "Episode 25000/50000 | Epsilon: 0.010\n",
      "Episode 30000/50000 | Epsilon: 0.010\n",
      "Episode 35000/50000 | Epsilon: 0.010\n",
      "Episode 40000/50000 | Epsilon: 0.010\n",
      "Episode 45000/50000 | Epsilon: 0.010\n",
      "Episode 50000/50000 | Epsilon: 0.010\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "agent = train(episodes=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67c9dda-d009-4315-b5b6-37212d452166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: 19 Wins, 1 Losses, 0 Draws\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "test(agent, games=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b14372-c42c-417f-9f8e-59bad18eefc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
